<h2> Lectures (Tom Mitchell and Maria-Florina Balcan) </h2>

<table class="schedule" border="1" cellspacing="0">
<tbody>
<tr>
<th>Lecture</th>
<th>Topics</th>
<th>Readings and useful links</th>
<th>Handouts</th>
</tr>
<tr>
<td>Intro to ML<br />Decision Trees</td>
<td>
<ul>
<li>Machine learning examples</li>
<li>Well defined machine learning problem</li>
<li>Decision tree learning</li>
</ul>
</td>
<td>Mitchell: Ch 3<br />Bishop: Ch 14.4<br /><a href="MachineLearning.pdf" data-smd-id="s15">The Discipline of Machine Learning</a></td>
<td><a href="01_DTreesAndOverfitting-1-12-2015.pdf" data-smd-id="s16">Slides</a></td>
</tr>
<tr>
<td>Decision Tree learning<br />Review of Probability</td>
<td>
<ul>
<li>The big picture</li>
<li>Overfitting</li>
<li>Random variables and probabilities</li>
</ul>
</td>
<td>Mitchell: Ch 3<br /><a href="prob18.pdf" data-smd-id="s18">Andrew Moore's Basic Probability Tutorial</a></td>
<td><a href="02_Overfitting_ProbReview-1-14-2015.pdf" data-smd-id="s19">Slides</a><br /><a href="02_Overfitting_ProbReview-1-14-2015_ann.pdf" data-smd-id="s20">Annotated Slides</a></td>
</tr>
<tr>
<td>Probability and Estimation</td>
<td>
<ul>
<li>Bayes rule</li>
<li>MLE</li>
<li>MAP</li>
</ul>
</td>
<td>Mitchell:&nbsp;<a href="Joint_MLE_MAP.pdf" data-smd-id="s22">Estimating Probabilities</a></td>
<td><a href="03_MLE_MAP_NBayes-1-21-2015.pdf" data-smd-id="s23">Slides</a><br /><a href="03_MLE_MAP_NBayes-1-21-2015_ann.pdf" data-smd-id="s24">Annotated Slides</a></td>
</tr>
<tr>
<td>Naive Bayes</td>
<td>
<ul>
<li>Conditional Independence</li>
<li>Naive Bayes: why and how</li>
</ul>
</td>
<td>Mitchell:&nbsp;<a href="NBayesLogReg.pdf" data-smd-id="s26">Naive Bayes and Logistic Regression</a></td>
<td><a href="04_NBayes-1-26-2015.pptx.pdf" data-smd-id="s27">Slides</a><br /><a href="04_NBayes-1-26-2015-ann.pdf" data-smd-id="s28">Annotated Slides</a></td>
</tr>
<tr>
<td>Gaussian Naive Bayes</td>
<td>
<ul>
<li>Gaussian Bayes classifiers</li>
<li>Document Classification</li>
<li>Brain image classification</li>
<li>Form of decision surfaces</li>
</ul>
</td>
<td>Mitchell:&nbsp;<a href="NBayesLogReg.pdf" data-smd-id="s30">Naive Bayes and Logistic Regression</a></td>
<td><a href="05_GNB_1-28-2015.pdf" data-smd-id="s31">Slides</a><br /><a href="05_GNB_1-28-2015-ann.pdf" data-smd-id="s32">Annotated Slides</a></td>
</tr>
<tr>
<td>Logistic Regression</td>
<td>
<ul>
<li>Naive Bayes - the big picture</li>
<li>Logistic Regression: Maximizing conditional likelihood</li>
<li>Gradient ascent as a general learning/optimization method</li>
</ul>
</td>
<td>Mitchell:&nbsp;<a href="NBayesLogReg.pdf" data-smd-id="s34">Naive Bayes and Logistic Regression</a></td>
<td><a href="06_GenDiscr_LR_2-2-2015.pdf" data-smd-id="s35">Slides</a><br /><a href="06_GenDiscr_LR_2-2-2015-ann.pdf" data-smd-id="s36">Annotated Slides</a></td>
</tr>
<tr>
<td>Linear Regression</td>
<td>
<ul>
<li>Generative/Discriminative models</li>
<li>Minimizing squared error and maximizing data likelihood</li>
<li>Regularization</li>
<li>Bias-variance decomposition</li>
</ul>
</td>
<td>&nbsp;</td>
<td><a href="07_GenDiscr2_2-4-2015.pdf" data-smd-id="s38">Slides</a><br /><a href="07_GenDiscr2_2-4-2015-ann.pdf" data-smd-id="s39">Annotated Slides</a></td>
</tr>
<tr>
<td>Learning Theory I</td>
<td>
<ul>
<li>Distributional Learning</li>
<li>PAC and Statistical Learning Theory</li>
<li>Sample Complexity</li>
</ul>
</td>
<td>Mitchell: Ch 7<br /><a href="sc-2015.pdf" data-smd-id="s41">Notes on Generalization Guarantees</a></td>
<td><a href="08_Theory_2-9-2015.pdf" data-smd-id="s42">Slides</a></td>
</tr>
<tr>
<td>Learning Theory II</td>
<td>
<ul>
<li>Sample Complexity</li>
<li>Shattering and VC Dimension</li>
<li>Sauer's Lemma</li>
</ul>
</td>
<td>Mitchell: Ch 7<br /><a href="sc-2015.pdf" data-smd-id="s44">Notes on Generalization Guarantees</a></td>
<td><a href="09_Theory2_2-11-2015.pdf" data-smd-id="s45">Slides</a></td>
</tr>
<tr>
<td>Learning Theory III</td>
<td>
<ul>
<li>Rademacher Complexity</li>
<li>Overfitting and Regularization</li>
</ul>
</td>
<td>&nbsp;</td>
<td><a href="10_Theory3_2-16-2015.pdf" data-smd-id="s47">Slides</a></td>
</tr>
<tr>
<td>Graphical Models I</td>
<td>
<ul>
<li>Bayes Nets</li>
<li>Representing joint distributions with conditional independence assumptions</li>
</ul>
</td>
<td>Bishop chapter 8, through 8.2</td>
<td><a href="11_GrMod1_2-18-2015.pdf" data-smd-id="s49">Slides</a><br /><a href="11_GrMod1_2-18-2015-ann.pdf" data-smd-id="s50">Annotated Slides</a></td>
</tr>
<tr>
<td>Graphical Models II</td>
<td>
<ul>
<li>Inference</li>
<li>Learning from fully observed data</li>
<li>Learning from partially observed data</li>
</ul>
</td>
<td>&nbsp;</td>
<td><a href="12_GrMod1_2-23-2015-ann.pdf" data-smd-id="s52">Annotated Slides</a></td>
</tr>
<tr>
<td>Graphical Models III</td>
<td>
<ul>
<li>EM</li>
<li>Semi-supervised learning</li>
</ul>
</td>
<td>Bishop Chapter 8<br />Mitchell Chapter 6</td>
<td><a href="13_GrMod3_2-25-2015.pdf" data-smd-id="s54">Slides</a><br /><a href="13_GrMod3_2-25-2015-ann.pdf" data-smd-id="s55">Annotated Slides</a></td>
</tr>
<tr>
<td colspan="4"><strong>Exam #1</strong></td>
</tr>
<tr>
<td>EM and Clustering</td>
<td>
<ul>
<li>Mixture of Gaussian clustering</li>
<li>K-means clustering</li>
</ul>
</td>
<td>Bishop Chapter 8<br />Mitchell Chapter 6</td>
<td><a href="14_GrMod4_3-4-2015.pdf" data-smd-id="s57">Slides</a><br /><a href="14_GrMod4_3-4-2015-ann.pdf" data-smd-id="s58">Annotated Slides</a></td>
</tr>
<tr>
<td colspan="4">Spring Break</td>
</tr>
<tr>
<td>Boosting</td>
<td>
<ul>
<li>Weak vs Strong (PAC) Learning</li>
<li>Boosting Accuracy</li>
<li>Adaboost</li>
</ul>
</td>
<td>
<ul>
<li><a href="schapire02boosting_schapire.pdf" data-smd-id="s60">The Boosting Approach to Machine Learning: An Overview</a></li>
<li><a href="nips-tutorial.pdf" data-smd-id="s61">Theory and Applications of Boosting (NIPS Tutorial)</a></li>
</ul>
</td>
<td><a href="15_boosting_3-16-2015.pdf" data-smd-id="s62">Slides</a></td>
</tr>
<tr>
<td>Adaboost, Margins, Perceptron</td>
<td>
<ul>
<li>Adaboost: Generalization Guarantees(naive and margins based).</li>
<li>Geometric Margins and Perceptron</li>
</ul>
</td>
<td><a href="perceptron-notes.pdf" data-smd-id="s64">Notes on Perceptron</a></td>
<td><a href="16_boosting-percepton-margins_03-18-2015.pdf" data-smd-id="s65">Slides</a><br /><a href="16_boosting-percepton-margins_03-18-2015.pptx" data-smd-id="s66">Slides (PPT)</a></td>
</tr>
<tr>
<td>Kernels</td>
<td>
<ul>
<li>Geometric Margins</li>
<li>Kernels: Kernelizing a Learning Algorithm</li>
<li>Kernelized Perceptron</li>
</ul>
</td>
<td>Bishop 6.1 and 6.2</td>
<td><a href="17_margins-kernels_03-23-2015.pdf" data-smd-id="s68">Slides</a></td>
</tr>
<tr>
<td>SVM</td>
<td>
<ul>
<li>Geometric Margins</li>
<li>SVM: Primal and Dual Forms</li>
<li>Kernelizing SVM</li>
<li>Semi-supervised Learning</li>
<li>Semi-supervised SVM</li>
</ul>
</td>
<td><a href="cs229-notes3.pdf" data-smd-id="s70">Notes on SVM by Andrew Ng</a></td>
<td><a href="18_svm-ssl_03-25-2015.pdf" data-smd-id="s71">Slides</a></td>
</tr>
<tr>
<td>Semi-supervised Learning</td>
<td>
<ul>
<li>Transductive SVM</li>
<li>Co-training and Multi-view Learning</li>
<li>Graph-based Methods</li>
</ul>
</td>
<td>
<ul>
<li><a href="SSL_EoML.pdf" data-smd-id="s73">"Semi-Supervised Learning" in Encyclopedia of Machine Learning</a></li>
<li><a href="joachims_99c.pdf" data-smd-id="s75">Transductive SVM Paper</a></li>
</ul>
</td>
<td><a href="19_ssl_03-30-2015.pdf" data-smd-id="s76">Slides</a></td>
</tr>
<tr>
<td>Active Learning</td>
<td>
<ul>
<li>Batch Active Learning</li>
<li>Selective Sampling and Active Learning</li>
<li>Sampling Bias</li>
</ul>
</td>
<td>
<ul>
<li><a href="twoface.pdf" data-smd-id="s78">Two Faces of Active Learning</a></li>
<li><a href="settles.activelearning.pdf" data-smd-id="s79">Active Learning Literature Survey (by Burr Settles)</a></li>
<li><a href="al-survey-enc-algos.pdf" data-smd-id="s80">Active Learning Survey (by Balcan and Urner)</a></li>
</ul>
</td>
<td><a href="20_al_4-1-2015.pdf" data-smd-id="s81">Slides</a></td>
</tr>
<tr>
<td>
<ul>
<li>Partitional Clustering</li>
<li>Hierarchical Clustering</li>
</ul>
</td>
<td>
<ul>
<li>k-means, Lloyd's method, k-means++</li>
<li>Agglomerative Clustering</li>
</ul>
</td>
<td>
<ul>
<li>Hastie, Tibshirani and Friedman, Chapter 14.3</li>
<li><a href="cluster-chapter.pdf" data-smd-id="s83">Center Based Clustering: A Foundational Perspective</a></li>
</ul>
</td>
<td><a href="21_clustering_4-6-2015.pdf" data-smd-id="s84">Slides</a></td>
</tr>
<tr>
<td>
<ul>
<li>Learning Representations</li>
<li>Dimensionality Reduction</li>
</ul>
</td>
<td>
<ul>
<li>Principal Component Analysis</li>
<li>Kernel Principal Component Analysis</li>
</ul>
</td>
<td>
<ul>Bishop 12.1, 12.3</ul>
</td>
<td><a href="22_pca-04-09-2015.pdf" data-smd-id="s86">Slides</a></td>
</tr>
<tr>
<td>Never Ending Learning</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><a href="23_nell_4-13-2015.pdf" data-smd-id="s88">Slides</a></td>
</tr>
<tr>
<td>Neural Networks<br />Deep Learning</td>
<td>&nbsp;</td>
<td>Mitchell, Chapter 4</td>
<td><a href="24_nn_4-15-2015.pdf" data-smd-id="s90">Slides</a></td>
</tr>
<tr>
<td>Reinforcement Learning</td>
<td>
<ul>
<li>Markov Decision Processes</li>
<li>Value Iteration</li>
<li>Q-learning</li>
</ul>
</td>
<td>
<ul>
<li>Mitchell, Chapter 13</li>
<li><a href="live-301-1562-jair.pdf" data-smd-id="s92">Kaelbling, et al., Reinforcement Learning: A Survey</a></li>
</ul>
</td>
<td><a href="25_rl_4-20-2015.pdf" data-smd-id="s93">Slides</a></td>
</tr>
<tr>
<td>Deep Learning<br />Differential Privacy<br />Discussion on the Future of ML</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><a href="26_privacy_4-22-2015.pdf" data-smd-id="s95">Slides (Privacy)</a><br /><a href="26_deep_4-22-2015.pdf" data-smd-id="s96">Slides (Deep Nets)</a></td>
</tr>
</tbody>
</table>
</br>

<p><strong>Andrew Ng's review notes on:</strong></p>
<ul>
<li><a href="1/cs229-linalg.pdf" target="_blank" rel="nofollow">Linear Algebra</a></li>
<li><a href="1/cs229-prob.pdf" target="_blank" rel="nofollow">Probability Theory</a></li>
<li><a href="1/gaussians.pdf" target="_blank" rel="nofollow">Multivariate Gaussian - I</a></li>
<li><a href="1/more_on_gaussians.pdf" target="_blank" rel="nofollow">Multivariate Gaussian - II</a></li>
<li><a href="1/cs229-cvxopt.pdf" target="_blank" rel="nofollow">Convex Optimization - I</a></li>
<li><a href="1/cs229-cvxopt2.pdf" target="_blank" rel="nofollow">Convex Optimization - II</a></li>
<li><a href="1/cs229-hmm.pdf" target="_blank" rel="nofollow">Hidden Markov Models</a></li>  
  
</ul>
</br>
<h2> Lecture Notes </h2>
<ul>
<li>Andrew Ng's&nbsp;<a href="1/loss-functions.pdf" target="_blank" rel="nofollow">notes on Loss Functions</a></li>
<li>Andrew Ng's course notes on&nbsp;<a href="1/cs229-notes1.pdf" target="_blank" rel="nofollow">Linear and Logistic Regression</a>.</li>
<li><a href="1/1905.12787.pdf" target="_blank" rel="nofollow">The Theory Behind Overfitting, Cross Validation, Regularization, Bagging and Boosting: Tutorial</a>&nbsp;by Ghojogh and Crowley.</li>
<li>Andrew Ng's notes on&nbsp;<a href="1/cs229-notes2.pdf" target="_blank" rel="nofollow">Naive Bayes</a>&nbsp;</li>
<li>Andrew Ng's notes on&nbsp;<a href="1/cs229-notes3.pdf" target="_blank" rel="nofollow">SVM and Kernel Methods</a></li>
<li>Andrew Ng's notes on&nbsp;<a href="1/cs229-notes-deep_learning.pdf" target="_blank" rel="nofollow">Neural Networks and Deep Learning</a></li>
<li>[Practical Tips] -&nbsp;<a href="1/lecun-98b.pdf" target="_blank" rel="nofollow">Efficient Backprop</a>&nbsp;by Yann LeCun</li>
<li>Vapnik's paper on "<a href="1/506-principles-of-risk-minimization-for-learning-theory.pdf" target="_blank" rel="nofollow">Principles of Risk Minimization for Learning Theory</a>", NIPS 1992</li>
 <li>Andrew Ng's &nbsp;<a href="machine-learning-4.2.pdf" target="_blank" rel="nofollow">ML Review Notes</a>&nbsp;</li> 
 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(1).pdf" style="text-decoration:none;">Spectral Methods for
Dimensionality Reduction</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(2).pdf" style="text-decoration:none;">Nonlinear Dimensionality
Reduction by Locally Linear Embedding</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(3).pdf" style="text-decoration:none;">MATLAB Workshop 2: An introduction to Support Vector Machine implementations in MATLAB</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(4).pdf" style="text-decoration:none;">Neighbourhood Components Analysis</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(5).pdf" style="text-decoration:none;">A Global Geometric Framework
for Nonlinear Dimensionality Reduction</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(6).pdf" style="text-decoration:none;">Graphics Principles Cheat Sheet</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(7).pdf" style="text-decoration:none;">Neural Machine Translation and Sequence-to-sequence Models: A Tutorial</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(8).pdf" style="text-decoration:none;"> Exponential Families </a></li>
   <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(9).pdf" style="text-decoration:none;">Variational Inference: A Review for Statisticians</a></li>
  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(10).pdf" style="text-decoration:none;">Foundations of Data Science </a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(11).pdf" style="text-decoration:none;">Optimization Methods for Large-Scale Machine Learning</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(12).pdf" style="text-decoration:none;">Convex Optimization</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(13).pdf" style="text-decoration:none;">Theoretical Computer Science Cheat Sheet</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(14).pdf" style="text-decoration:none;">Entropy, Relative Entropy, And Mutual Information</a></li>
                              
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(15).pdf" style="text-decoration:none;">Information Theory
And Statistics</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(16).pdf" style="text-decoration:none;">Lecture #7: Understanding and Using Principal
Component Analysis (PCA)</a></li>

  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(17).pdf" style="text-decoration:none;">
Lecture #9: The Singular Value Decomposition (SVD) and Low-Rank Matrix Approximations</a></li>   
  
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(18).pdf" style="text-decoration:none;">CS224n: Natural Language Processing with Deep
Learning (Lecture Notes: Part I)</a></li> 

  
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(19).pdf" style="text-decoration:none;">Super VIP Cheatsheet: Machine Learning</a></li> 

<li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(20).pdf" style="text-decoration:none;">
CSE176 Introduction to Machine Learning — Lecture notes</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(21).pdf" style="text-decoration:none;">Linear Algebra Review and Reference</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(22).pdf" style="text-decoration:none;">Lecture notes (Roger Grosse)</a></li> 
 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(23).pdf" style="text-decoration:none;">Data Mining, Inference, and Prediction</a></li> 
 

   <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(24).pdf" style="text-decoration:none;">A Primer on Neural Network Models
for Natural Language Processing</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(25).pdf" style="text-decoration:none;">Machine Learning and Data Mining
Lecture Notes</a></li>                              
 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(26).pdf" style="text-decoration:none;">Common tests are linear models</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(27).pdf" style="text-decoration:none;">
Linear Algebra Abridged</a></li>
   
 
   <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(28).pdf" style="text-decoration:none;">The Matrix Calculus You Need For Deep Learning</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(29).pdf" style="text-decoration:none;">Measure, Integration
and Real Analysis</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(30).pdf" style="text-decoration:none;">Concise Machine Learning</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(31).pdf" style="text-decoration:none;">Machine Learning Basic Concepts</a></li> 
    <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(32).pdf" style="text-decoration:none;">
Notation: Overview</a></li> 

   <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(33).pdf" style="text-decoration:none;">Lecture Notes 1: Vector spaces</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(34).pdf" style="text-decoration:none;">Probability Cheatsheet</a></li> 
 
  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(35).pdf" style="text-decoration:none;">An overview of gradient descent optimization
algorithms</a></li> 

  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(36).pdf" style="text-decoration:none;">Lecture 0: Course Introduction</a></li> 
 
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(37).pdf" style="text-decoration:none;">A Structural Approach to Selection Bias</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(38).pdf" style="text-decoration:none;">Troubleshooting Deep Neural Networks: A Field Guide to Fixing Your Model</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(39).pdf" style="text-decoration:none;">Introduction to
Applied Linear Algebra</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(40).pdf" style="text-decoration:none;">Python For Data Science Cheat Sheet (NumPy Basics)</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(41).pdf" style="text-decoration:none;">Data Wrangling with pandas Cheat Sheet</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(42).pdf" style="text-decoration:none;">Python For Data Science Cheat Sheet
(Scikit-Learn)</a></li>
 
  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(43).pdf" style="text-decoration:none;">Python For Data Science Cheat Sheet
(Keras)</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(44).pdf" style="text-decoration:none;">Data Wrangling with dplyr and tidyr Cheat Sheet</a></li>
   <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(45).pdf" style="text-decoration:none;">Data Visualization with ggplot2
Cheat Sheet</a></li>  
   
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(46).pdf" style="text-decoration:none;">CS725 : Foundations of Machine learning - Lecture Notes</a></li> 
                             
 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(47).pdf" style="text-decoration:none;">Lecture Notes: Optimization for Machine Learning</a></li> 
 
  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(1145).pdf" style="text-decoration:none;">CS446: Machine Learning Lecture Notes</a></li> 
 
 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(96).pdf" style="text-decoration:none;">Data Science Lecture Notes</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(100).pdf" style="text-decoration:none;">Lectures on Numerical Methods for Data Science</a></li>
 
 
 
</ul>

</br>


<h3>C19 ML lectures [Andrew Zisserman]</h3>
<ul>
<li>
<p><a href="1/lect1.pdf">Lecture 1 "Introduction"</a></p>
</li>
<li>
<p><a href="1/lect2.pdf">Lecture 2 "The SVM Classifier"</a></p>
</li>
<li>
<p><a href="1/lect3.pdf">Lecture 3 "SVM dual, kernels and regression"</a></p>
</li>
<li>
<p><a href="1/lect4.pdf">Lecture 4 "Regression continued and multiple classes"</a></p>
</li>
<li><a href="1/examples.pdf">Examples sheet</a></li>
</ul>

</br>
<table border="2" cellpadding="0" bgcolor="lightyellow">
<tbody>
<tr>
<td align="CENTER">
<h1>ML Lecture Notes</h1>
<p>(Prof. Qiangfu Zhao, Prof. Yong Liu and Prof. Yuichi Yaguchi)</p>
</td>
</tr>
<tr>
<td align="CENTER">Contents</td>
</tr>
<tr>
<td><a href="2/Lec01.pdf" data-smd-id="s3">History of AI and ML</a></td>
</tr>
<tr>
<td><a href="2/Lec02.pdf" data-smd-id="s4">Fundamentals of machine learning</a></td>
</tr>
<tr>
<td><a href="2/Lec03.pdf" data-smd-id="s5">Introduction to concept learning</a></td>
</tr>
<tr>
<td><a href="2/Lec04.pdf" data-smd-id="s6">Basic statistic learning</a></td>
</tr>
<tr>
<td><a href="2/Lec05.pdf" data-smd-id="s7">Bayesian network</a></td>
</tr>
<tr>
<td><a href="2/project-1.pdf" data-smd-id="s8">Project-I</a></td>
</tr>
<tr>
<td><a href="2/Lec06.pdf" data-smd-id="s9">Multilayer perceptron</a></td>
</tr>
<tr>
<td><a href="2/Lec07.pdf" data-smd-id="s10">Convolutional neural network</a></td>
</tr>
<tr>
<td><a href="2/Lec08.pdf" data-smd-id="s11">Autoencoder</a></td>
</tr>
<tr>
<td><a href="2/Lec09.pdf" data-smd-id="s12">Restricted Boltzmann machine</a></td>
</tr>
<tr>
<td><a href="2/Lec10.pdf" data-smd-id="s13">Decision trees</a></td>
</tr>
<tr>
<td><a href="2/Project-II.pdf" data-smd-id="s14">Project-II</a></td>
</tr>
</tbody>
</table>
</br>

<h2>Statistical ML Lecture Notes [Thomas Schön]</h2> 
<ul>
<li><a href="2/le1.pdf">[pdf]</a>&nbsp;<strong>Introduction</strong>&nbsp;(<a href="2/schonl2011.pdf">notes</a>)</li>
<li><strong><a href="2/le2.pdf">[pdf]</a>&nbsp;Linear regression</strong>&nbsp;</li>
<li><strong><a href="2/le3.pdf">[pdf]</a>&nbsp;Linear classification</strong>&nbsp;</li>
<li><strong><a href="2/le4.pdf">[pdf]</a>&nbsp;Neural networks, kernel methods intro.</strong>&nbsp;</li>
<li><strong><a href="2/le5.pdf">[pdf]</a>&nbsp;Kernel methods&nbsp;</strong></li>
<li><strong><a href="2/le6.pdf">[pdf]</a>&nbsp;EM and clustering</strong>&nbsp;(<a href="2/schonem2009.pdf">Notes</a>)</li>
<li><strong><a href="2/le7.pdf">[pdf]</a>&nbsp;Approximate inference</strong>&nbsp;(<a href="2/vbexample.pdf">Notes</a>)</li>
<li><strong><a href="2/le8.pdf">[pdf]</a>&nbsp;Graphical models</strong>&nbsp;</li>
<li><strong><a href="2/le9.pdf">[pdf]&nbsp;</a>Graphical models and message passing</strong>&nbsp;</li>
<li><strong><a href="2/le10.pdf">[pdf]</a>&nbsp;MCMC and sampling methods</strong>&nbsp;</li>
<li><strong><a href="2/le11.pdf">[pdf]</a>&nbsp;Bayesian nonparametric models</strong>&nbsp;</li>
</ul>

</br>

<h2> Papers </h2>
<ul>
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(48).pdf" style="text-decoration:none;">Discrete Adversarial Attacks and Submodular Optimization with Applications to Text Classification</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(49).pdf" style="text-decoration:none;">Towards Deep Learning Models Resistant to Adversarial Attacks</a></li>
                              
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(50).pdf" style="text-decoration:none;">Synthesizing Robust Adversarial Examples</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(51).pdf" style="text-decoration:none;">Robust Physical-World Attacks on Deep Learning Visual Classification</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(52).pdf" style="text-decoration:none;">Audio Adversarial Examples: Targeted Attacks on Speech-to-Text</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(53).pdf" style="text-decoration:none;">PoTrojan: powerful neuron-level trojan designs in
deep learning models</a></li>
 
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(54).pdf" style="text-decoration:none;">Semantic Adversarial Examples </a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(55).pdf" style="text-decoration:none;">Programmable Neural Network Trojan for Pre-Trained Feature Extractor</a></li>
 
  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(56).pdf" style="text-decoration:none;">STRIP: A Defence Against Trojan Attacks on Deep
Neural Networks </a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(57).pdf" style="text-decoration:none;">A General Framework for Adversarial Examples
with Objectives</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(58).pdf" style="text-decoration:none;">Adversarial Attacks on Neural Network Policies</a></li>
    <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(59).pdf" style="text-decoration:none;">Adversarial Examples Are Not Easily Detected:
Bypassing Ten Detection Methods</a></li>
 
  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(60).pdf" style="text-decoration:none;">Adversarial Examples that Fool both Computer
Vision and Time-Limited Humans</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(61).pdf" style="text-decoration:none;">Analyzing Federated Learning through an Adversarial Lens</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(62).pdf" style="text-decoration:none;">Neural Cleanse: Identifying and Mitigating
Backdoor Attacks in Neural Networks</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(63).pdf" style="text-decoration:none;">Generating Natural Language Adversarial Examples</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(64).pdf" style="text-decoration:none;">Sparse Bayesian Adversarial Learning Using Relevance Vector Machine Ensembles</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(65).pdf" style="text-decoration:none;">ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness</a></li> 

   <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(66).pdf" style="text-decoration:none;">Imperceptible, Robust, and Targeted
Adversarial Examples for Automatic Speech Recognition</a></li> 
 
   <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(67).pdf" style="text-decoration:none;">Poison Frogs! Targeted Clean-Label Poisoning
Attacks on Neural Networks</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(68).pdf" style="text-decoration:none;">Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples</a></li> 
 
  
   <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(69).pdf" style="text-decoration:none;">On Evaluating Adversarial Robustness</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(70).pdf" style="text-decoration:none;">Adversarial Support Vector Machine Learning</a></li> 
  
 
 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(71).pdf" style="text-decoration:none;">Trojaning Attack on Neural Networks</a></li>
 
 
 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(72).pdf" style="text-decoration:none;"> Learning Conjunctive Concepts in Structural Domains</a></li> 
 
 
 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(73).pdf" style="text-decoration:none;">Reducing Multiclass to Binary:
A Unifying Approach for Margin Classifiers</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(74).pdf" style="text-decoration:none;">Approximation and estimation bounds for artificial neural networks</a></li>
    <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(75).pdf" style="text-decoration:none;">Training a 3-Node Neural Network is
NP-Complete</a></li>                        
<li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(76).pdf" style="text-decoration:none;">Online Passive-Aggressive Algorithms</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(77).pdf" style="text-decoration:none;">Adaptive Subgradient Methods for
Online Learning and Stochastic Optimization</a></li> 
 
 
 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(78).pdf" style="text-decoration:none;">Quantifying Inductive Bias:
AI Learning Algorithms and Valiant's Learning Framework</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(79).pdf" style="text-decoration:none;">The Perceptron algorithm versus Winnow:
linear versus logarithmic mistake bounds when few input variables are relevant</a></li>


 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(80).pdf" style="text-decoration:none;">Large Margin Classification
Using the Perceptron Algorithm</a></li> 
 
 
 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(81).pdf" style="text-decoration:none;">Learning Quickly When Irrelevant Attributes
Abound: A New Linear-threshold Algorithm</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(82).pdf" style="text-decoration:none;">General Convergence Results for Linear
Discriminant Updates</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(83).pdf" style="text-decoration:none;">Learning representations by back-propagating errors</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(84).pdf" style="text-decoration:none;">Efficient Learning of Linear Perceptrons</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(85).pdf" style="text-decoration:none;">On the Computational Efficiency of Training Neural Networks</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(86).pdf" style="text-decoration:none;">Learnability and the Vapnik-Chervonenkis Dimension</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(87).pdf" style="text-decoration:none;">Computational Limitations on Learning from Examples</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(88).pdf" style="text-decoration:none;">Learning Decision Lists</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(89).pdf" style="text-decoration:none;">A Theory of the Learnable</a></li>
  
  
  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(90).pdf" style="text-decoration:none;">On the Uniform Convergence of Relative Frequencies of Events to Their Probabilities</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(91).pdf" style="text-decoration:none;">On-Line Algorithms in Machine Learning</a></li>

   <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(92).pdf" style="text-decoration:none;">Neural Programmer-Interpreters</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(93).pdf" style="text-decoration:none;">Dueling Network Architectures for Deep Reinforcement Learning</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(94).pdf" style="text-decoration:none;">scikit-learn user guide</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(95).pdf" style="text-decoration:none;">The Matrix Cookbook</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(97).pdf" style="text-decoration:none;">Big Data: New Tricks for Econometrics</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(98).pdf" style="text-decoration:none;">Crash Course on Basic Statistics</a></li>

  <li><a target="_blank" href="https://github.com/manjunath5496/ML-Lectures/blob/master/ml(99).pdf" style="text-decoration:none;">Think Stats: Probability and
Statistics for Programmers</a></li>
 
 
 
</ul>

</br>
<p><a><strong>ML Lecture Slides </strong>[</a><a href="http://www.cedar.buffalo.edu/~srihari">Sargur Srihari</a>]</p><hr>
<table border="0">
<tbody>
<tr>
<td rowspan="2">
<ol>
<ol>
<li><strong><span style="color: red;">Introduction</span></strong>
<ol>
<li><span style="color: #000099;"><a href="3/1.1%20ML-Overview.pdf">Machine Learning-Overview</a></span>(28MB)</li>
<li><span style="color: #000099;"><a href="3/1.2%20SoftwareFrameworks.pdf">Python and ML Frameworks</a></span>(13.9MB)</li>
<li><span style="color: #000099;"><a href="3/1.3%20LinearAlgebra.pdf">Linear Algebra</a></span>(4.5MB)&nbsp;</li>
<li><span style="color: #000099;"><a href="3/1.4%20Curve-Fitting.pdf">Example: Curve Fitting</a></span>(934KB)</li>
<li><span style="color: #000099;"><a href="3/1.5%20Probability-Theory.pdf">Probability Theory</a></span>(4.9MB)</li>
<li><span style="color: #000099;"><a href="3/NumericalComputation.pdf">Numerical Computation</a></span>(1.4MB)</li>
<li><span style="color: #000099;"><a href="3/1.7%20Decision-Theory.pdf">Decision-Theory</a></span>(488KB)</li>
<li><span style="color: #000099;"><a href="3/1.8%20Information-Theory.pdf">Information Theory</a></span>(715KB)</li>
</ol>
</li>
<li><strong><span style="color: red;">Probability Distributions</span></strong>
<ol>
<li><span style="color: #000099;"><a href="3/Ch2.1-Discrete-Distributions.pdf">Discrete Distributions</a></span>(1MB)</li>
<li><span style="color: #000099;"><a href="3/Ch2.2-Gaussian-Distribution.pdf">Gaussian Distribution</a></span>(833KB)</li>
<li><span style="color: #000099;"><a href="3/8.5%20GaussianBNs.pdf">Gaussian Bayesian Networks</a></span>(738KB)</li>
</ol>
</li>
<li><strong><span style="color: red;">Linear Models for Regression</span></strong>
<ol>
<li><span style="color: #000099;"><a href="3/3.1-Regression-BasisFns.pdf">Regression with Basis Functions</a></span>(7.3MB)</li>
<li><span style="color: #000099;"><a href="3/3.2-GradientDescent.pdf">Gradient Descent</a></span>(3.2MB)</li>
<li><span style="color: #000099;"><a href="3/3.3-Bias-Variance.pdf">Bias-Variance</a></span>(950KB)</li>
<li><span style="color: #000099;"><a href="3/3.4-BayesianRegression.pdf">Bayesian Regression</a></span>(2.5MB)</li>
<li><span style="color: #000099;"><a href="3/3.5-BayesianModelComparison.pdf">Bayesian Model Comparison</a></span>(478KB)</li>
<li><span style="color: #000099;"><a href="3/3.6-EvidenceApproximation.pdf">Evidence Approximation</a></span>(746KB)</li>
<li><span style="color: #000099;"><a href="3/3.7-CS-Ranking.pdf">Example: Computer Science Ranking</a></span>(126KB)</li>
</ol>
</li>
<li><strong><span style="color: red;">Linear Models for Classification</span></strong>
<ol start="0">
<li><span style="color: #000099;"><a href="3/4.0%20Overview.pdf">Overview</a></span>(4.6MB)</li>
<li><span style="color: #000099;"><a href="3/4.1%20DiscFns.pdf">Discriminant Functions</a></span>(5MB)</li>
<li><span style="color: #000099;"><a href="3/4.2%20Generative.pdf">Probabilistic Generative Models</a></span>(1.3MB)</li>
<li><span style="color: #000099;">Probabilistic Discriminative Models</span>
<ol>
<li><span style="color: #000099;"><a href="3/4.3.1-FixedBasis.pdf">Fixed Basis Functions</a></span>(254KB)</li>
<li><span style="color: #000099;"><a href="3/4.3.2-LogisticReg.pdf">Logistic Regression</a></span>(3.6MB)</li>
<li><span style="color: #000099;"><a href="3/4.3.3-IRLS.pdf">Iterative Reweighted Least Squares</a></span>(5.1MB)</li>
<li><span style="color: #000099;"><a href="3/4.3.4-MultiLogistic.pdf">Multiclass Logistic Regression</a></span>(4.6MB)</li>
<li><span style="color: #000099;"><a href="3/4.3.5-ProbitRegression.pdf">Probit Regression</a></span>(356KB)</li>
<li><span style="color: #000099;"><a href="3/4.3.6-CanonicalLink.pdf">Canonical Link Functions</a></span>(263KB)</li>
</ol>
</li>
<li><span style="color: #000099;"><a href="3/4.4-Laplace.pdf">Laplace Approximation&nbsp;</a></span>(1.3MB)</li>
<li><span style="color: #000099;"><a href="3/4.5.1-BayesLogistic.pdf">Bayesian Logistic Regression</a></span>(1.1MB)</li>
<li><span style="color: #000099;"><a href="3/4.5.2-VarBayesLogistic.pdf">Variational Bayesian Logistic Regression</a></span>(3.3MB)&nbsp;</li>
</ol>
</li>
<li><strong><span style="color: red;">Neural Networks</span></strong>
<ol start="0&quot;">
<li><span style="color: #000099;"><a href="3/Chap5.0-Biology.pdf">Biology</a></span>(4.5MB)&nbsp;</li>
<li><span style="color: #000099;"><a href="3/Chap5.1-FeedFor.pdf">Feed-forward Network Functions</a></span>(5.3MB)</li>
<li><span style="color: #000099;"><a href="3/Chap5.2-Training.pdf">Network Training</a></span>(2.6MB)</li>
<li><span style="color: #000099;"><a href="3/Chap5.3-BackProp.pdf">Backpropagation</a></span>(8.7MB)</li>
<li><span style="color: #000099;"><a href="3/Chap5.4-Hessian.pdf">The Hessian Matrix</a></span>(562KB)</li>
<li><strong><span style="color: red;">Regularization in Neural Networks</span></strong>
<ol>
<li><span style="color: #000099;"><a href="3/Chap5.5-Regularization.pdf">Norm Penalty: Bayesian Interpretation</a></span>(1.2MB)</li>
<li><span style="color: #000099;"><a href="3/Chap5.5.6-ConvolutionalNetworks.pdf">Convolutional Networks</a></span>(4.9MB)</li>
<li><span style="color: #000099;"><a href="3/Chap5.5.7-SoftWtSharing.pdf">Soft Weight Sharing</a></span>(1.2MB)</li>
</ol>
</li>
<li><span style="color: #000099;"><a href="3/Chap5.6-MixDensityNetworks.pdf">Mixture Density Networks&nbsp;</a></span>(634KB)</li>
<li><span style="color: #000099;"><a href="3/Chap5.7-BayesianNeuralNetworks.pdf">Bayesian Neural Networks</a></span>(716KB)</li>
<li><span style="color: #000099;"><a href="3/Chap5.8-DeepLearning.pdf">Deep Learning Overview</a></span>(5.2MB)</li>
</ol>
</li>
<li><strong><span style="color: red;">Kernel Methods</span></strong>
<ol>
<li><span style="color: #000099;"><a href="3/Chap6.1-KernelMethods.pdf">Kernel Methods</a></span>(6.3MB)</li>
<li><span style="color: #000099;"><a href="3/Chap6.2-RadialBasisFunctions.pdf">Radial Basis Function Networks</a></span>(812KB)</li>
<li><span style="color: #000099;"><a href="3/Chap6.3-GaussianProcesses.pdf">Gaussian Processes</a></span>(6.8MB)</li>
</ol>
</li>
<li><strong><span style="color: red;">Sparse Kernel Machines</span></strong>
<ol>
<li><span style="color: #000099;"><a href="3/7.1-SVMs.pdf">Support Vector Machines</a></span>(5.4MB)</li>
<li><span style="color: #000099;"><a href="3/7.2-SVM-Overlap.pdf">SVM for Overlapping Distributions</a></span>(1.3MB)</li>
<li><span style="color: #000099;"><a href="3/7.3-SVM-Multiclass.pdf">Multiclass SVMs&nbsp;</a></span>(1.4MB)</li>
<li><span style="color: #000099;"><a href="3/7.4-SVM-LogisticReg.pdf">Relation to Logistic Regression&nbsp;</a></span>(446KB)</li>
</ol>
</li>
<li><strong><span style="color: red;">Mixture Models and EM</span></strong>
<ol start="0">
<ol start="0">
<li><span style="color: #000099;"><a href="3/Ch9.0-UnsupervisedLearning.pdf">Unsupervised Learning</a></span>(1.9MB)</li>
</ol>
</ol>
</li>
<li><span style="color: #000099;"><a href="3/Ch9.1-Kmeans.pdf">K-means Clustering</a></span>(1.4MB)</li>
<li><span style="color: #000099;"><a href="3/Ch9.2-MixturesofGaussians.pdf">Gaussian Mixture Models</a></span>(1.5MB)</li>
<li><span style="color: #000099;"><a href="3/Ch9.3-LatentVariableViewofEM.pdf">Latent Variable View of EM</a></span>(1.1MB)</li>
<li><span style="color: #000099;"><a href="3/Ch9.4-MixturesofBernoulli.pdf">Bernoulli Mixture Models</a></span>(3.1MB)</li>
<li><span style="color: #000099;"><a href="3/Ch9.5-TheoreticBasisEM.pdf">Theoretical Basis of EM</a></span>(693KB)</li>
</ol>
</ol>
<ul>
<li><strong><span style="color: red;">Approximate Inference</span></strong>
<ol>
<li><span style="color: #000099;"><a href="3/10.1ApproximateInference.pdf">Approximate Inference</a></span>(180KB)</li>
<li><span style="color: #000099;"><a href="3/10.2VariationalInference.pdf">Variational Inference</a></span>(3.3MB)</li>
<li><span style="color: #000099;"><a href="3/10.3VariationalGMM.pdf">Variational Mixture of Gaussians</a></span>(1MB)</li>
</ol>
</li>
<li><strong><span style="color: red;">Sampling Methods</span></strong>
<ol>
<li><span style="color: #000099;"><a href="3/Ch11.1-NeedForSampling.pdf">Need for Sampling&nbsp;</a></span>(6.6MB)</li>
<li><span style="color: #000099;"><a href="3/Ch11.2-BasicSampling.pdf">Basic Sampling Methods</a></span>(2.5MB)</li>
<li><span style="color: #000099;"><a href="3/Ch11.3-MCMCSampling.pdf">Markov Chain Monte Carlo Sampling</a></span>(815KB)</li>
<li><span style="color: #000099;"><a href="3/Ch11.4-Gibbs%20Sampling.pdf">Gibbs Sampling</a></span>(1.2MB)</li>
</ol>
</li>
<li><strong><span style="color: red;">Sequential Data</span></strong>
<ol>
<li><span style="color: #000099;"><a href="3/13.1-MarkovModels.pdf">Markov Models</a></span>(2.5MB)</li>
<li><span style="color: #000099;"><a href="3/13.2-HMMs.pdf">Hidden Markov Models</a></span>(3.1MB)</li>
<ol>
<li><span style="color: #000099;"><a href="3/13.2.1-MLE%20for%20HMM.pdf">Maximum Likelihood for the HMM</a></span>(8.5MB)</li>
<li><span style="color: #000099;"><a href="3/13.2.2-ForwardBackward.pdf">The forward-backward algorithm</a></span>(15.9MB)</li>
<li><span style="color: #000099;"><a href="3/Ch13.3-HMMExtensions.pdf">Extensions to HMMs</a></span>(287KB)</li>
</ol>
<li><span style="color: #000099;"><a href="3/Ch13.4-LinearDynamicalSystems.pdf">Linear Dynamical Systems</a></span>(217KB)</li>
<li><span style="color: #000099;"><a href="3/Ch13.5-ConditionalRandomFields.pdf">Conditional Random Fields</a></span>(1.6MB)</li>
</ol>
</li>
<li><strong><span style="color: red;">Combining Models</span></strong>
<ol>
<li><a href="3/14.1%20CombiningModels.pdf">Combining Models</a>(pdf, 1.7MB)</li>
<li><a href="3/14.2%20Bagging.pdf"><span style="color: #000099;">Bagging</span></a>(pdf, 675KB)</li>
<li><a href="3/14.3%20Boosting.pdf"><span style="color: #000099;">Boosting</span></a>(pdf, 1.1MB)</li>
<li><span style="color: #000099;">Tree Models</span>
<ol>
<li><a href="3/14.4.1%20Decision%20Trees.pdf"><span style="color: #000099;">Decision Trees</span></a>(pdf, 1.9MB)</li>
<li><a href="3/14.4.2%20Learning%20Trees.pdf"><span style="color: #000099;">Learning Trees</span></a>(pdf, 596KB)</li>
</ol>
</li>
<li><a href="3/14.6%20Random%20Forests.pdf"><span style="color: #000099;">Random Forests</span></a>(pdf, 3.4MB)</li>
</ol>
</li>
<li><strong><span style="color: red;">Reinforcement Learning</span></strong>
<ol>
<li><a href="3/15.1-Reinf-Learning.pdf"><span style="color: #000099;">Reinforcement Learning Overview</span></a>(pdf 4MB)</li>
<li><a href="3/15.2-LearningTask.pdf"><span style="color: #000099;">The Learning Task&nbsp;</span></a>(pdf 1MB)</li>
<li><a href="3/15.3-Q-Learning.pdf"><span style="color: #000099;">Q-Learning</span></a>&nbsp;(pdf 6MB)</li>
<li><a href="3/15.4-NondeterministicQ.pdf"><span style="color: #000099;">Nondeterministic Q-Learning</span></a>&nbsp;(pdf 4.9MB)</li>
<li><a href="3/15.5-DeepReinforcement.pdf"><span style="color: #000099;">Deep Reinforcement Learning</span></a>&nbsp;(pdf 3.5MB)</li>
</ol>
</li>
<li><strong><span style="color: red;">AI Ethics</span></strong>
<ol>
<li><a href="3/16.1%20Ethics%20of%20AI.pdf"><span style="color: #000099;">AI Ethics</span></a>(pdf 8.8MB)</li>
</ol>
</li>
<li><strong><span style="color: red;">Trustworthy AI</span></strong>
<ol>
<li><a href="3/17.1%20Trustworthy%20AI.pdf"><span style="color: #000099;">Trustworthy AI</span></a>(pdf 2.9MB)</li>
<li><a href="3/17.2.1%20Explainable%20AI.pdf"><span style="color: #000099;">Explainable AI</span></a>(pdf 24MB)</li>
<ol>
<li><a href="3/17.2.2%20TeachingExplanation.pdf"><span style="color: #000099;">Explanation by Example</span></a>(pdf 3.4MB)</li>
<li><a href="3/17.2.3%20DeepExplanation.pdf"><span style="color: #000099;">Deep Explanation</span></a>(pdf 14.6MB)</li>
<li><a href="3/17.2.4%20CausalExplanation.pdf"><span style="color: #000099;">Causal Explanation</span></a>(pdf 16.4MB)</li>
</ol>
</ol>
</li>
<li><strong>Concept Learning</strong>
<ol>
<li><span style="color: #000099;"><a href="3/Chap18.1-Hypotheses.pdf">Hypothesis Space</a>&nbsp;</span>(pdf, 111KB)</li>
<li><span style="color: #000099;"><a href="3/Chap18.2-CandidateElimination.pdf">Candidate Elimination</a></span>&nbsp;(pdf,236KB)</li>
</ol>
</li>
<li><strong>Computational Learning Theory</strong>
<ol>
<li><a href="3/Chap19.Part1.pdf"><span style="color: #000099;">PAC Learning</span></a>(pdf, 98KB)</li>
<li><a href="3/Chap19.Part2.pdf"><span style="color: #000099;">VC Dimension</span></a>(pdf, 321KB)</li>
<li><a href="3/Chap19.Part3.pdf"><span style="color: #000099;">Mistake Bound</span></a>(pdf, 51KB)</li>
</ol>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
</br>
<p><strong>ML Course Notes [</strong><a href="http://www.cs.ox.ac.uk/people/nando.defreitas/" data-smd-id="s28">Nando de Freitas</a><strong>] </strong></p><hr>
<ul>
<li><strong>Lecture 1</strong>: Introduction&nbsp;<a href="3/lecture1.pdf" data-smd-id="s29">slides</a>&nbsp;</li>
<li><strong>Lecture 2</strong>: Linear prediction&nbsp;<a href="3/lecture2.pdf" data-smd-id="s31">slides</a>&nbsp;</li>
<li><strong>Lecture 3</strong>: Maximum likelihood&nbsp;<a href="3/lecture3.pdf" data-smd-id="s33">slides.pdf</a>&nbsp;</li>
<li><strong>Lectures 4 &amp; 5</strong>: Regularizers, basis functions and cross-validation&nbsp;<a href="3/lecture4.pdf" data-smd-id="s35">slides.pdf</a>&nbsp;</li>
<li><strong>Lecture 6</strong>: Optimisation&nbsp;<a href="3/lecture5.pdf" data-smd-id="s38">slides.pdf</a>&nbsp;</li>
<li><strong>Lecture 7</strong>: Logistic regression&nbsp;<a href="3/lecture6.pdf" data-smd-id="s40">slides.pdf</a>&nbsp;</li>
<li><strong>Lecture 8</strong>: Back-propagation and layer-wise design of neural nets&nbsp;<a href="3/lecture7.pdf" data-smd-id="s42">slides.pdf</a>&nbsp;</li>
<li><strong>Lecture 9</strong>: Neural networks and deep learning with Torch&nbsp;<a href="3/lecture8.pdf" data-smd-id="s44">slides.pdf</a>&nbsp;</li>
<li><strong>Lecture 10</strong>: Convolutional neural networks&nbsp;<a href="3/lecture9.pdf" data-smd-id="s46">slides.pdf</a>&nbsp;</li>
<li><strong>Lecture 11</strong>: Max-margin learning and siamese networks&nbsp;<a href="3/lecture10.pdf" data-smd-id="s48">slides.pdf</a>&nbsp;</li>
<li><strong>Lecture 12</strong>: Recurrent neural networks and LSTMs&nbsp;<a href="3/lecture11.pdf" data-smd-id="s50">slides.pdf</a>&nbsp;</li>
<li><strong>Lecture 15</strong>: Reinforcement learning with direct policy search&nbsp;<a href="3/lecture12.pdf" data-smd-id="s52">slides.pdf</a>&nbsp;</li>
<li><strong>Lecture 16</strong>: Reinforcement learning with action-value functions&nbsp;<a href="3/lecture12.pdf" data-smd-id="s54">slides.pdf</a>&nbsp;</li>
<li><strong>Practical on week 2</strong>: (1) Learning Lua and the tensor library.&nbsp;<a href="3/practical1.pdf" data-smd-id="s58">pdf</a></li>
<li><strong>Practical on week 3</strong>: (2) Online and batch linear regression.&nbsp;<a href="3/practical2.pdf" data-smd-id="s59">pdf</a></li>
<li><strong>Practical on week 4</strong>: (3) Logistic regression and optimization.&nbsp;<a href="3/practical3.pdf" data-smd-id="s60">pdf</a></li>
<li><strong>Practical on week 6</strong>: (4) Feedforward neural networks, and implementing your own layer.&nbsp;<a href="3/practical4.pdf" data-smd-id="s61">pdf</a></li>
<li><strong>Practical on week 7</strong>: (5) Intro to nngraph for graph-shaped modules.&nbsp;<a href="3/practical5.pdf" data-smd-id="s62">pdf</a></li>
<li><strong>Practical on week 8</strong>: (6) Training a LSTM language model.&nbsp;<a href="3/practical6.pdf" data-smd-id="s63">pdf</a></li>
<li><strong>Class on Week 3</strong>:&nbsp;<a href="3/class1.pdf" data-smd-id="s65">Problem set</a>.&nbsp;</li>
<li><strong>Class on Week 5</strong>:&nbsp;<a href="3/class2.pdf" data-smd-id="s66">Problem set</a>.&nbsp;</li>
<li><strong>Class on Week 7</strong>:&nbsp;<a href="3/class3.pdf" data-smd-id="s67">Problem set</a>.&nbsp;</li>
<li><strong>Class on Week 8</strong>:&nbsp;<a href="3/class4.pdf" data-smd-id="s68">Problem set</a>.&nbsp;</li>
</ul>

</br>
<h2>Lectures on Machine Learning (<a href="https://www.cs.ubc.ca/~schmidtm/"><strong>Mark Schmidt</strong></a>)</h2>
<h4>1. Supervised Learning</h4>
<ul>
<li><a href="4/overview.pdf" data-smd-id="s4">Overview</a></li>
<li><a href="4/L2.pdf" data-smd-id="s5">Exploratory Data Analysis</a></li>
<li><a href="4/L3.pdf" data-smd-id="s6">Decision Trees</a>&nbsp;(<a href="4/bigO.pdf" data-smd-id="s7">Notes on Big-O Notation</a>)</li>
<li><a href="4/L4.pdf" data-smd-id="s8">Fundamentals of Learning</a>&nbsp;(<a href="4/notation.pdf" data-smd-id="s9">Notation Guide</a>)</li>
<li><a href="4/L5.pdf" data-smd-id="s10">Probabilistic Classifiers</a>&nbsp;(<a href="4/probabilitySlides.pdf" data-smd-id="s11">Probability Slides</a>,&nbsp;<a href="4/probability.pdf" data-smd-id="s12">Notes on Probability</a>)</li>
<li><a href="4/L6.pdf" data-smd-id="s13">Non-Parametric Models</a></li>
<li><a href="4/L7.pdf" data-smd-id="s14">Ensemble Methods</a></li>
</ul>
<h4>2. Unsupervised Learning</h4>
<ul>
<li><a href="4/L8.pdf" data-smd-id="s15">Clustering</a></li>
<li><a href="4/L9.pdf" data-smd-id="s16">More Clustering</a></li>
<li><a href="4/L10.pdf" data-smd-id="s17">Outlier Detection</a></li>
<li><a href="4/L11.pdf" data-smd-id="s18">Finding Similar Items</a></li>
</ul>
<h4>3. Linear Models</h4>
<ul>
<li><a href="4/L12.pdf" data-smd-id="s19">Least Squares</a>&nbsp;(<a href="4/calculus.pdf" data-smd-id="s20">Notes on Calculus</a>,&nbsp;<a href="4/2009_Notes_LinearAlgebra.pdf" data-smd-id="s21">Notes on Linear Algebra</a>,&nbsp;<a href="4/linearQuadraticGradients.pdf" data-smd-id="s22">Notes on Linear/Quadratic Gradients</a>)</li>
<li><a href="4/L13.pdf" data-smd-id="s23">Nonlinear Regression</a></li>
<li><a href="4/L14.pdf" data-smd-id="s24">Gradient Descent</a></li>
<li><a href="4/L15.pdf" data-smd-id="s25">Robust Regression</a></li>
<li><a href="4/L16.pdf" data-smd-id="s26">Feature Selection</a></li>
<li><a href="4/L17.pdf" data-smd-id="s27">Regularization</a></li>
<li><a href="4/L18.pdf" data-smd-id="s28">More Regularization</a></li>
<li><a href="4/L19.pdf" data-smd-id="s29">Linear Classifiers</a></li>
<li><a href="4/L20.pdf" data-smd-id="s30">More Linear Classifiers</a></li>
<li><a href="4/L21.pdf" data-smd-id="s31">Feature Engineering</a></li>
<li><a href="4/L22.pdf" data-smd-id="s32">Convolutions</a></li>
<li><a href="4/L23.pdf" data-smd-id="s33">Kernel Methods</a></li>
<li><a href="4/L24.pdf" data-smd-id="s34">Stochastic Gradient</a></li>
<li><a href="4/L25.pdf" data-smd-id="s35">Boosting</a></li>
<li><a href="4/L26.pdf" data-smd-id="s36">MLE and MAP</a>&nbsp;(<a href="4/max.pdf" data-smd-id="s37">Notes on Max and Argmax</a>)</li>
</ul>
<h4>4. Latent-Factor Models</h4>
<ul>
<li><a href="4/L27.pdf" data-smd-id="s38">Principal Component Analysis</a></li>
<li><a href="4/L28.pdf" data-smd-id="s39">More PCA</a></li>
<li><a href="4/L29.pdf" data-smd-id="s40">Sparse Matrix Factorization</a></li>
<li><a href="4/L30.pdf" data-smd-id="s41">Recommender Systems</a></li>
<li><a href="4/L31.pdf" data-smd-id="s42">Nonlinear Dimensionality Reduction</a></li>
</ul>
<h4>5. Deep Learning</h4>
<ul>
<li><a href="4/L32.pdf" data-smd-id="s43">Deep Learning</a></li>
<li><a href="4/L33.pdf" data-smd-id="s44">More Deep Learning</a></li>
<li><a href="4/L34.pdf" data-smd-id="s45">Convolutional Neural Networks</a></li>
<li><a href="4/L35b.pdf" data-smd-id="s46">More CNNs</a></li>
</ul>
<h3>Part 2: Data Science 573 and 575</h3>
<ul>
<li><a href="4/structLearn.pdf" data-smd-id="s47">Structure Learning</a></li>
<li><a href="4/sequenceMining.pdf" data-smd-id="s48">Sequence Mining</a></li>
<li><a href="4/20201007.pdf" data-smd-id="s49">Tensor Basics</a></li>
<li><a href="4/semiSupervised.pdf" data-smd-id="s50">Semi-Supervised Learning</a></li>
<li><a href="4/pageRank.pdf" data-smd-id="s51">PageRank</a></li>
<li><a href="4/MCandMC.pdf" data-smd-id="s52">Markov Chains and Monte Carlo</a></li>
</ul>
<h3>Part 3: Computer Science 540</h3>
<h4>A. Fundamentals</h4>
<ul>
<li><a href="4/L1.pdf" data-smd-id="s54">340 Overview</a></li>
<li><a href="4a/L2.pdf" data-smd-id="s55">Fundamentals of Learning</a></li>
<li><a href="4a/L3.pdf" data-smd-id="s56">Convexity</a>&nbsp;(<a href="4/norms.pdf" data-smd-id="s57">Notes on Norms</a>)</li>
<li><a href="4/L4a.pdf" data-smd-id="s58">How Much Data?</a></li>
</ul>
<h4>B. Density Estimation</h4>
<ul>
<li><a href="4/L4b.pdf" data-smd-id="s59">Structured Prediction Motivation</a></li>
<li><a href="4a/L5.pdf" data-smd-id="s60">Density Estimation</a></li>
<li><a href="4a/L6.pdf" data-smd-id="s61">Multivariate Gaussians</a></li>
<li><a href="4a/L7.pdf" data-smd-id="s62">Mixture Models</a></li>
<li><a href="4a/L8.pdf" data-smd-id="s63">Generative Classifiers</a></li>
<li><a href="4a/L9.pdf" data-smd-id="s64">Expectation Maximization</a>&nbsp;(<a href="4/EM.pdf" data-smd-id="s65">Notes on EM</a>)</li>
<li><a href="4a/L10.pdf" data-smd-id="s66">Kernel Density Estimation</a></li>
<li><a href="4/L17.5.pdf" data-smd-id="s67">Probabilistic PCA, Factor Analysis, Independent Component Analysis</a></li>
</ul>
<h4>C. Graphical Models</h4>
<ul>
<li><a href="4a/L11.pdf" data-smd-id="s68">Markov Chains</a></li>
<li><a href="4a/L12.pdf" data-smd-id="s69">Monte Carlo Methods</a></li>
<li><a href="4a/L13.pdf" data-smd-id="s70">Message Passing</a></li>
<li><a href="4a/L14.pdf" data-smd-id="s71">Hidden Markov Models</a></li>
<li><a href="4a/L15.pdf" data-smd-id="s72">DAG Models</a></li>
<li><a href="4a/L16.pdf" data-smd-id="s73">More DAGs</a></li>
<li><a href="4a/L17.pdf" data-smd-id="s74">Undirected Graphical Models</a></li>
<li><a href="4a/L18.pdf" data-smd-id="s75">Approximate Inference</a></li>
<li><a href="4a/L19.pdf" data-smd-id="s76">Log-Linear Models</a></li>
<li><a href="4a/L20.pdf" data-smd-id="s77">Boltzmann Machines</a></li>
<li><a href="4a/L21.pdf" data-smd-id="s78">Conditional Random Fields</a></li>
<li><a href="4/L28.5.pdf" data-smd-id="s79">Structured SVMs</a></li>
</ul>
<h4>D. Bayesian Learning</h4>
<ul>
<li><a href="4a/L25.pdf" data-smd-id="s80">Bayesian Statistics</a></li>
<li><a href="4a/L26.pdf" data-smd-id="s81">Empirical Bayes</a></li>
<li><a href="4a/L27.pdf" data-smd-id="s82">Conjguate Prios</a></li>
<li><a href="4a/L28.pdf" data-smd-id="s83">Hierarchical Bayes</a></li>
<li><a href="4a/L29.pdf" data-smd-id="s84">Topics Models</a></li>
<li><a href="4a/L30.pdf" data-smd-id="s85">Rejection/Importance Sampling</a></li>
<li><a href="4a/L31.pdf" data-smd-id="s86">Metropolis-Hastings</a></li>
<li><a href="4a/L34.pdf" data-smd-id="s87">Variational Inference</a></li>
<li><a href="4/L35.pdf" data-smd-id="s88">Non-Parametric Bayes</a></li>
<li><a href="4/NP.pdf" data-smd-id="s89">Infinite Mixture Models</a></li>
</ul>
<h4>E. More Deep Learning</h4>
<ul>
<li><a href="4a/L22.pdf" data-smd-id="s90">Neural Networks</a></li>
<li><a href="4/L22b.pdf" data-smd-id="s91">Double Descent Curves</a></li>
<li><a href="4a/L23.pdf" data-smd-id="s92">Deep Structured Models</a></li>
<li><a href="4a/L24.pdf" data-smd-id="s93">Fully-Convolutional Networks</a></li>
<li><a href="4/L24b.pdf" data-smd-id="s94">Recurrent Neural Networks</a></li>
<li><a href="4a/L32.pdf" data-smd-id="s95">Long Short Term Memory</a></li>
<li><a href="4a/L33.pdf" data-smd-id="s96">Faster Algorithms for Deep Learning?</a></li>
<li><a href="4/L36.pdf" data-smd-id="s97">VAEs and GANs</a></li>
<li><a href="4/L37.pdf" data-smd-id="s98">Attention</a></li>
</ul>
<h3>Part 4: Large-Scale Machine Learning</h3>
<ul>
<li><a href="4/L3.pdf" data-smd-id="s99">Convex Optimization</a>&nbsp;(<a href="4/norms.pdf" data-smd-id="s100">Notes on Norms</a>)</li>
<li><a href="4/S1.pdf" data-smd-id="s101">Gradient Descent Progress</a>&nbsp;(<a href="4/convex.pdf" data-smd-id="s102">Notes on Convexity Inequalities</a>,&nbsp;<a href="4/differentiable.pdf" data-smd-id="s103">Notes on Implementing Gradient Descent</a>)</li>
<li><a href="4/S2.pdf" data-smd-id="s104">Gradient Descent Convergence</a></li>
<li><a href="4/S3.pdf" data-smd-id="s105">Linear and Superlinear Convergence</a></li>
<li><a href="4/S4.pdf" data-smd-id="s106">Subgradient Methods</a></li>
<li><a href="4/S5.pdf" data-smd-id="s107">Projected-Gradient</a></li>
<li><a href="4/S6.pdf" data-smd-id="s108">Proximal-Gradient</a></li>
<li><a href="4/S7.pdf" data-smd-id="s109">Structured Regularization</a></li>
<li><a href="4/S8.pdf" data-smd-id="s110">Coordinate Optimization</a></li>
<li><a href="4/mirrorMultiLevel.pdf" data-smd-id="s111">Mirror Descent and Multi-Level Methods</a></li>
<li><a href="4/S9.pdf" data-smd-id="s112">Randomized Algorithms</a></li>
<li><a href="4/S10.pdf" data-smd-id="s113">Stochastic Subgradient</a></li>
<li><a href="4/S11.pdf" data-smd-id="s114">Variance-Reduced Stochastic Gradient</a></li>
<li><a href="4/L12.5.pdf" data-smd-id="s115">Kernel Methods and Fenchel Duality</a></li>
<li><a href="4/S12.pdf" data-smd-id="s116">Online Learning</a></li>
<li><a href="4/S13.pdf" data-smd-id="s117">Over-Parameterized Models</a></li>
</ul>
<h3>Part 5: Machine Learning Reading Group</h3>
<ul>
<li><a href="4/parallelDistributed.pdf" data-smd-id="s119">Parallel and Distributed Machine Learning</a></li>
<li><a href="4/onlineActiveCausal.pdf" data-smd-id="s120">Online, Active, and Causal Learning</a></li>
<li><a href="4/reinforcementLearning.pdf" data-smd-id="s121">Reinforcement Learning</a></li>
<li><a href="4/assorted.pdf" data-smd-id="s122">Overview of Other Large/Notable Topics</a></li>
</ul>
</br>
<h2>Lecture Notes on Data Analysis, Statistics, and Machine Learning (<a href="https://www.cs.uic.edu/~wilkinson/index.html">Leland Wilkinson</a>)</h2>
<ul>
<li><a href="Session%2001%20(Introduction).pdf">Introduction</a></li>
<li><a href="Session%2002%20(Data).pdf">Data</a></li>
<li><a href="Session%2003%20(Visualizing).pdf">Visualizing</a></li>
<li><a href="Session%2004%20(Exploring).pdf">Exploring</a></li>
<li><a href="Session%2005%20(Summarizing).pdf">Summarizing</a></li>
<li><a href="Session%2006%20(Distributions).pdf">Distributions</a></li>
<li><a href="Session%2007%20(Inference).pdf">Inference</a></li>
<li><a href="Session%2008%20(Predicting).pdf">Predicting</a></li>
<li><a href="Session%2009%20(Smoothing).pdf">Smoothing</a></li>
<li><a href="Session%2010%20(Time%20Series).pdf">Time Series</a></li>
<li><a href="Session%2011%20(Comparing).pdf">Comparing</a></li>
<li><a href="Session%2012%20(Grouping).pdf">Grouping</a></li>
<li><a href="Session%2013%20(Reducing).pdf">Reducing</a></li>
<li><a href="Session%2014%20(Learning).pdf">Learning</a></li>
<li><a href="Session%2015%20(Anomalies).pdf">Anomalies</a></li>
<li><a href="Session%2016%20(Analyzing).pdf">Analyzing</a></li>
</ul>


